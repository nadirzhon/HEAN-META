# ============================================
# HEAN Monitoring Stack (Standalone)
# ============================================
# Purpose: Dedicated monitoring compose file for development/testing
# Usage:
#   make monitoring-up      # Start monitoring
#   make monitoring-down    # Stop monitoring
#
# Services:
# - Prometheus: Metrics collection (port 9091)
# - Alertmanager: Alert routing + Telegram (port 9093)
# - Grafana: Visualization (port 3001)
# ============================================

version: '3.9'

services:
  # ============================================
  # Prometheus - Metrics Collection
  # ============================================
  prometheus:
    image: prom/prometheus:latest
    container_name: hean-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.listen-address=:9090'
    volumes:
      - ./infra/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./infra/monitoring/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9091:9090"
    restart: unless-stopped
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  # ============================================
  # Alertmanager - Alert Routing + Telegram
  # ============================================
  alertmanager:
    image: prom/alertmanager:latest
    container_name: hean-alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - ./infra/monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    ports:
      - "9093:9093"
    environment:
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-0}
    restart: unless-stopped
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # Grafana - Visualization
  # ============================================
  grafana:
    image: grafana/grafana:latest
    container_name: hean-grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: redis-datasource
      GF_SERVER_ROOT_URL: http://localhost:3001
    volumes:
      - grafana-data:/var/lib/grafana
      - ./infra/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./infra/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    ports:
      - "3001:3000"
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # Jaeger - Distributed Tracing (OTEL all-in-one)
  # ============================================
  # Receives traces via OTLP gRPC (port 4317) and HTTP (port 4318).
  # UI available at http://localhost:16686
  # Enable tracing: OTEL_ENABLED=true OTEL_ENDPOINT=http://localhost:4317
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: hean-jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      # Keep traces for 72 hours in the in-memory store (default: 8h)
      - SPAN_STORAGE_TYPE=memory
      - MEMORY_MAX_TRACES=100000
    ports:
      - "16686:16686"   # Jaeger UI
      - "4317:4317"     # OTLP gRPC receiver (used by otel.py OTLPSpanExporter)
      - "4318:4318"     # OTLP HTTP receiver (alternative transport)
      - "14268:14268"   # Jaeger Thrift HTTP collector (legacy clients)
    restart: unless-stopped
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

# ============================================
# Networks
# ============================================
networks:
  monitoring:
    driver: bridge
    name: hean-monitoring

# ============================================
# Persistent Volumes
# ============================================
volumes:
  prometheus-data:
    driver: local
    name: hean-prometheus-data
  alertmanager-data:
    driver: local
    name: hean-alertmanager-data
  grafana-data:
    driver: local
    name: hean-grafana-data
