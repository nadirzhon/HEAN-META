# HEAN Technology Infrastructure Guide
**World-Class HFT Infrastructure Optimization Roadmap**

Date: 2026-02-06
Target: Achieve top-tier performance for crypto trading on Bybit

---

## Executive Summary

This guide provides a comprehensive roadmap for upgrading HEAN's technology infrastructure from its current Python/asyncio foundation to world-class HFT performance standards. Based on analysis of HEAN's codebase and research into leading HFT firms (Citadel Securities, Jump Trading, Wintermute, DRW), this document outlines specific, actionable improvements prioritized by impact and feasibility.

**Current State**: HEAN is a well-architected event-driven Python trading system with:
- FastAPI backend with async/await throughout
- Multi-priority EventBus (CRITICAL/NORMAL/LOW queues)
- Bybit WebSocket connections for market data and private updates
- Redis for state management
- Docker containerization
- HTTP REST API for order execution

**Performance Gap**: Crypto exchanges operate in millisecond (ms) latency ranges, not microseconds. HEAN's current architecture is optimized for this reality but can achieve significant improvements.

---

## 1. LATENCY OPTIMIZATION

### 1.1 Current HEAN Latency Bottlenecks (Identified from Code Analysis)

**EventBus Architecture**:
- **Current**: Multi-priority asyncio queues with fast-path for CRITICAL events (SIGNAL, ORDER_REQUEST, ORDER_FILLED)
- **Bottleneck**: Even fast-path events go through `_dispatch_fast()` â†’ `_dispatch()` â†’ handler loop
- **Impact**: ~100-500 microseconds per event routing cycle

**WebSocket Processing**:
- **Current**: Python `websockets` library with SSL context, JSON parsing via standard `json` module
- **Bottleneck**:
  - SSL handshake and decryption overhead
  - Standard `json.loads()` is 3-5x slower than alternatives
  - No connection pooling or keep-alive optimization
- **Impact**: ~5-20ms WebSocket message processing latency

**Order Execution Flow**:
- **Current**: Signal â†’ EventBus â†’ ExecutionRouter â†’ BybitHTTPClient â†’ httpx request
- **Bottleneck**:
  - Each stage adds 50-200Î¼s overhead
  - HTTP request/response cycle: 10-50ms (network-bound)
  - Signature generation (HMAC SHA256) on every request: ~100-300Î¼s
- **Impact**: Total tick-to-trade: 20-80ms (mostly network)

**Market Data Pipeline**:
- **Current**: WebSocket â†’ JSON parse â†’ Tick creation â†’ EventBus publish â†’ Strategy handler
- **Bottleneck**:
  - JSON parsing dominates CPU time
  - Object creation overhead (Tick dataclass instantiation)
  - No orderbook caching or incremental updates
- **Impact**: ~1-5ms per tick processing

### 1.2 Python-Specific Optimizations (Highest ROI)

#### 1.2.1 Drop-in Performance Upgrades

**Replace asyncio with uvloop** â­â­â­â­â­
```python
# In src/hean/main.py or src/hean/api/main.py
import uvloop
uvloop.install()  # Single line, 2-4x throughput improvement
```
- **Impact**: 2-4x throughput for async operations, 40-60% latency reduction
- **Effort**: 1 line of code
- **Risk**: Very low (drop-in replacement)
- **Source**: [uvloop documentation](https://github.com/MagicStack/uvloop)

**Replace json with orjson for WebSocket parsing** â­â­â­â­â­
```python
# In src/hean/exchange/bybit/ws_public.py and ws_private.py
import orjson  # instead of json
data = orjson.loads(message)  # 20-50% faster
```
- **Impact**: 20-50% faster JSON deserialization
- **Effort**: 5-10 lines of code changes
- **Risk**: Very low (compatible API)
- **Cost**: `pip install orjson`

**Use msgpack for internal EventBus serialization** â­â­â­â­
```python
# For Redis state or inter-process communication
import msgpack
# 5-10x faster than JSON, 50% smaller payloads
```
- **Impact**: 5-10x faster serialization for internal messages
- **Effort**: Moderate (requires schema changes)
- **Risk**: Low (only for internal use)

#### 1.2.2 Connection Pooling & Keep-Alive

**Optimize httpx client configuration** â­â­â­â­
```python
# In src/hean/exchange/bybit/http.py
self._client = httpx.AsyncClient(
    timeout=10.0,
    limits=httpx.Limits(
        max_keepalive_connections=20,  # Reuse connections
        max_connections=50,
        keepalive_expiry=30.0,
    ),
    http2=True,  # Enable HTTP/2 multiplexing
)
```
- **Impact**: 30-50% latency reduction on repeated requests
- **Effort**: 10 lines of code
- **Risk**: Low

**WebSocket Connection Tuning** â­â­â­
```python
# In ws_public.py and ws_private.py
ssl_context = ssl.create_default_context()
ssl_context.minimum_version = ssl.TLSVersion.TLSv1_3  # Faster handshake
ssl_context.set_ciphers('ECDHE+AESGCM')  # Fastest cipher suite

# Enable compression for bandwidth reduction
self._websocket = await websockets.connect(
    ws_url,
    ssl=ssl_context,
    compression=None,  # Disable if CPU-bound, enable if bandwidth-bound
    max_size=10_000_000,  # Larger buffer for high-volume feeds
)
```

#### 1.2.3 EventBus Optimization

**Inline Critical Event Handlers** â­â­â­â­
```python
# Instead of routing through queues, directly invoke handlers for CRITICAL events
class EventBus:
    def __init__(self):
        self._fast_handlers: dict[EventType, list[Callable]] = {}

    async def publish(self, event: Event):
        # Direct invocation for ultra-critical events (no queue)
        if event.event_type in self._fast_handlers:
            await asyncio.gather(*[h(event) for h in self._fast_handlers[event.event_type]])
        else:
            # Regular queue-based routing
            queue, priority = self._get_queue_for_event(event)
            await queue.put(event)
```
- **Impact**: Reduce SIGNAL â†’ ORDER latency by 50-80%
- **Effort**: Moderate refactoring
- **Risk**: Medium (changes core event flow)

**Remove Thread Pool for Async Handlers** â­â­â­
```python
# Current code uses ThreadPoolExecutor for sync handlers
# Recommendation: Convert ALL handlers to async
# This eliminates thread context switching overhead (~50-100Î¼s per call)
```

### 1.3 WebSocket Optimization Deep Dive

#### 1.3.1 Orderbook Reconstruction

**Implement Incremental Orderbook Updates** â­â­â­â­â­
```python
class OrderBookManager:
    def __init__(self):
        self._books: dict[str, OrderBook] = {}

    def handle_delta(self, symbol: str, delta: dict):
        """Apply incremental delta instead of full snapshot"""
        book = self._books[symbol]
        # Update only changed levels (10-100x faster than full rebuild)
        for bid in delta.get('bids', []):
            book.update_bid(float(bid[0]), float(bid[1]))
        for ask in delta.get('asks', []):
            book.update_ask(float(ask[0]), float(ask[1]))
```
- **Impact**: 10-100x faster orderbook updates
- **Effort**: High (requires stateful orderbook management)
- **Risk**: Medium (synchronization complexity)

**Pre-allocate Orderbook Arrays** â­â­â­
```python
import numpy as np

class OrderBook:
    def __init__(self, max_depth: int = 200):
        # Pre-allocate fixed-size arrays (avoid reallocation)
        self.bids = np.zeros((max_depth, 2), dtype=np.float64)  # [price, size]
        self.asks = np.zeros((max_depth, 2), dtype=np.float64)
        self.bid_count = 0
        self.ask_count = 0
```
- **Impact**: 5-10x faster level updates
- **Effort**: Moderate
- **Risk**: Low

#### 1.3.2 WebSocket Reconnection Strategy

**Current State**: HEAN has basic reconnection with exponential backoff
**Improvement**: Add connection health monitoring and preemptive reconnection

```python
class BybitPublicWebSocket:
    async def _health_monitor(self):
        """Preemptively reconnect on degraded connection"""
        while self._connected:
            await asyncio.sleep(5.0)

            # Monitor message rate
            if self._last_message_time and (time.time() - self._last_message_time > 10.0):
                logger.warning("WebSocket stale, preemptive reconnect")
                await self._reconnect()

            # Monitor latency (if exchange provides timestamps)
            if hasattr(self, '_avg_latency') and self._avg_latency > 100:  # >100ms
                logger.warning(f"WebSocket latency degraded: {self._avg_latency}ms")
                # Consider switching to backup connection
```

### 1.4 Order Submission Latency Reduction

**Signature Pre-computation** â­â­â­
```python
class BybitHTTPClient:
    def __init__(self):
        self._signature_cache: dict[str, tuple[str, float]] = {}  # (sig, timestamp)

    def _get_or_compute_signature(self, canonical: str) -> str:
        """Cache signatures for repeated requests (e.g., GET /positions)"""
        cache_key = hashlib.sha256(canonical.encode()).hexdigest()[:16]
        if cache_key in self._signature_cache:
            sig, ts = self._signature_cache[cache_key]
            if time.time() - ts < 4.0:  # 4s cache TTL (within 5s recvWindow)
                return sig

        # Compute fresh signature
        sig = self._sign_request(...)
        self._signature_cache[cache_key] = (sig, time.time())
        return sig
```
- **Impact**: 100-300Î¼s saved on repeated requests
- **Effort**: Moderate
- **Risk**: Low (with proper TTL)

**Order Request Batching** â­â­â­â­
```python
# Bybit supports batch orders in single HTTP request
async def submit_batch_orders(self, orders: list[OrderRequest]) -> list[Order]:
    """Submit up to 10 orders in single request (amortize network latency)"""
    batch_data = {
        "category": "linear",
        "requests": [self._format_order(o) for o in orders]
    }
    response = await self._request("POST", "/v5/order/create-batch", data=batch_data)
    # Single 20ms network round-trip instead of N Ã— 20ms
```

### 1.5 Market Data Processing Pipeline

**Parallelize Strategy Evaluation** â­â­â­â­
```python
# Current: Strategies process ticks sequentially
# Improvement: Parallel strategy evaluation

class TradingSystem:
    async def _handle_tick(self, tick: Tick):
        # Broadcast tick to all strategies in parallel
        tasks = [strategy.on_tick(tick) for strategy in self._strategies]
        signals = await asyncio.gather(*tasks, return_exceptions=True)

        # Process signals
        for signal in signals:
            if isinstance(signal, Signal):
                await self._bus.publish(Event(EventType.SIGNAL, signal))
```
- **Impact**: N strategies evaluated in parallel instead of sequential
- **Effort**: Low (already async)
- **Risk**: Very low

**Feature Computation Caching** â­â­â­
```python
from functools import lru_cache

class Strategy:
    @lru_cache(maxsize=1000)
    def _compute_indicator(self, price_tuple: tuple, period: int) -> float:
        """Cache expensive indicator calculations"""
        prices = list(price_tuple)
        return self._ema(prices, period)
```

---

## 2. ARCHITECTURE IMPROVEMENTS

### 2.1 Memory Management for Real-Time Data

**Current State**: Python garbage collection can cause 10-50ms pauses
**Impact**: Latency spikes during GC cycles

#### 2.1.1 GC Tuning

```python
# In main.py startup
import gc

# Disable automatic GC, run manually during idle periods
gc.disable()

async def gc_manager():
    """Manual GC during low-activity periods"""
    while True:
        await asyncio.sleep(60.0)  # Every minute

        # Only GC if market is quiet
        if time.time() - last_tick_time > 5.0:
            gc.collect(generation=0)  # Collect only young objects
```
- **Impact**: Eliminate 10-50ms GC pauses
- **Effort**: Low
- **Risk**: Medium (requires monitoring)

#### 2.1.2 Object Pooling

```python
class TickPool:
    """Pre-allocate Tick objects to avoid GC pressure"""
    def __init__(self, size: int = 10000):
        self._pool = [Tick(symbol="", price=0.0, timestamp=0) for _ in range(size)]
        self._index = 0

    def get_tick(self, symbol: str, price: float, timestamp: float) -> Tick:
        tick = self._pool[self._index]
        tick.symbol = symbol
        tick.price = price
        tick.timestamp = timestamp
        self._index = (self._index + 1) % len(self._pool)
        return tick
```

### 2.2 Multi-Process Architecture for Scaling

**Current State**: Single-process Python (GIL-bound for CPU-heavy work)
**Recommendation**: Multi-process architecture for different components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       HEAN Multi-Process                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Feed Handlerâ”‚â—„â”€â”€â”€â”¤    Redis     â”œâ”€â”€â”€â–ºâ”‚ Strategy Procâ”‚ â”‚
â”‚  â”‚   (Process 1)â”‚    â”‚  (Shared Mem)â”‚    â”‚  (Process 2) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                         â”‚         â”‚
â”‚         â”‚                                         â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          Execution Router (Process 3)                 â”‚ â”‚
â”‚  â”‚        (Receives signals, routes orders)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
```python
# Use Python multiprocessing with shared memory
from multiprocessing import Process, Queue
import multiprocessing.shared_memory as shm

# Process 1: Feed Handler (dedicated to WebSocket I/O)
# Process 2: Strategy Engine (dedicated to signal generation)
# Process 3: Execution Router (dedicated to order submission)
# Communication: Redis pub/sub or shared memory ring buffers
```

- **Impact**: Eliminate GIL contention, full CPU utilization
- **Effort**: High (major refactoring)
- **Risk**: High (complexity)

### 2.3 Async Task Scheduling Optimization

**Prioritize Order Submission Over Market Data** â­â­â­â­
```python
# Use asyncio task priorities (Python 3.12+)
import asyncio

async def main():
    loop = asyncio.get_running_loop()

    # High priority: Order execution
    order_task = loop.create_task(execute_order(), priority=10)

    # Low priority: Market data processing
    tick_task = loop.create_task(process_tick(), priority=1)
```

---

## 3. INFRASTRUCTURE

### 3.1 Cloud Provider Selection for Minimum Latency to Bybit

**Research Findings**:
- Bybit servers are primarily located in **Singapore** (AWS ap-southeast-1) and **Tokyo** (AWS ap-northeast-1)
- For testnet: Same regions as mainnet

**Optimal Setup** â­â­â­â­â­:
```
Region: AWS ap-southeast-1 (Singapore)
Instance Type: c7gn.xlarge (Graviton3, network-optimized)
Expected Latency to Bybit: <1ms RTT

Configuration:
- Enhanced networking enabled (SR-IOV)
- Placement group for lowest latency
- EBS-optimized for fast disk I/O
- Elastic IP (avoid DNS lookups)
```

**Cost-Benefit**:
- c7gn.xlarge: ~$0.20/hour = $144/month
- Latency improvement: 10-50ms â†’ <1ms (if currently running from US/EU)
- **ROI**: Critical for market making (adverse selection cost >> hosting cost)

### 3.2 Container Optimization for Trading

**Current Dockerfile** (from analysis):
- Uses `python:3.11-slim`
- Installs gcc, g++, git
- Multi-stage build

**Optimizations** â­â­â­â­:

```dockerfile
# Use smaller, faster base image
FROM python:3.11-slim-bookworm AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc g++ git && \
    rm -rf /var/lib/apt/lists/*

# Pre-compile Python bytecode
RUN pip install --no-cache-dir compileall2
COPY src/ /app/src/
RUN python -m compileall2 -f /app/src

# Runtime stage (smaller)
FROM python:3.11-slim-bookworm

# Copy only runtime dependencies
COPY --from=builder /app /app
COPY requirements.txt /app/

# Use uvloop and orjson
RUN pip install --no-cache-dir -r /app/requirements.txt uvloop orjson

# Optimize Python runtime
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONHASHSEED=0 \
    MALLOC_TRIM_THRESHOLD_=100000

# Use faster malloc (jemalloc)
RUN apt-get update && apt-get install -y --no-install-recommends libjemalloc2
ENV LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2

CMD ["python", "-m", "hean.main"]
```

- **Impact**: 20-30% faster startup, 10-15% memory reduction
- **Effort**: Moderate
- **Risk**: Low

### 3.3 Network Configuration (TCP Tuning)

**Docker Host (Linux)** â­â­â­â­:
```bash
# /etc/sysctl.conf optimizations
# Increase TCP buffer sizes
net.core.rmem_max = 134217728          # 128MB receive buffer
net.core.wmem_max = 134217728          # 128MB send buffer
net.ipv4.tcp_rmem = 4096 87380 67108864  # TCP receive buffer
net.ipv4.tcp_wmem = 4096 65536 67108864  # TCP send buffer

# Enable TCP fast open
net.ipv4.tcp_fastopen = 3

# Reduce TIME_WAIT sockets
net.ipv4.tcp_fin_timeout = 15
net.ipv4.tcp_tw_reuse = 1

# Increase connection backlog
net.core.somaxconn = 4096
net.ipv4.tcp_max_syn_backlog = 8192

# Apply: sudo sysctl -p
```

**DNS Caching** â­â­â­:
```python
# Pre-resolve Bybit endpoints to avoid DNS lookups
BYBIT_ENDPOINTS = {
    "api-testnet.bybit.com": "52.77.xxx.xxx",  # Cached IP
    "stream-testnet.bybit.com": "52.77.xxx.xxx",
}

# Use IP directly in connection
async def connect():
    ip = BYBIT_ENDPOINTS["api-testnet.bybit.com"]
    # Connect to IP, set Host header
```

### 3.4 Redis Optimization for State Management

**Current Setup**: Basic Redis with default config
**Optimizations** â­â­â­â­:

```bash
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru

# Disable persistence for speed (if state can be rebuilt)
save ""
appendonly no

# Network optimization
tcp-backlog 511
tcp-keepalive 60

# Performance
lazyfree-lazy-eviction yes
lazyfree-lazy-expire yes

# Use Redis pipelining for batch operations
```

```python
# Use Redis pipelining
import redis.asyncio as redis

async def batch_update_positions(positions: list[Position]):
    pipe = redis_client.pipeline()
    for pos in positions:
        pipe.hset(f"position:{pos.symbol}", mapping=asdict(pos))
    await pipe.execute()  # Single round-trip for N operations
```

### 3.5 Monitoring and Alerting for Latency Spikes

**Prometheus Metrics** â­â­â­â­â­:
```python
# Add latency histograms for critical paths
from prometheus_client import Histogram

tick_processing_latency = Histogram(
    'tick_processing_seconds',
    'Time to process tick',
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]  # 1ms to 1s
)

order_submission_latency = Histogram(
    'order_submission_seconds',
    'Time from signal to order submission',
    buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1.0]  # 10ms to 1s
)

@tick_processing_latency.time()
async def process_tick(tick: Tick):
    # ... processing
```

**Grafana Dashboard**:
- P50, P95, P99 latencies for tick processing
- Order submission latency trends
- WebSocket message rate and gaps
- EventBus queue depths and drops
- GC pause frequency and duration

---

## 4. DATA PIPELINE

### 4.1 Market Data Normalization Pipeline

**Current State**: Ad-hoc parsing in WebSocket handlers
**Improvement**: Centralized normalization layer

```python
class MarketDataNormalizer:
    """Normalize data from different exchanges to standard format"""

    def normalize_bybit_tick(self, raw: dict) -> Tick:
        """Fast path for Bybit â†’ internal format"""
        return Tick(
            symbol=raw['symbol'],
            price=float(raw['lastPrice']),
            timestamp=int(raw['time']),
            volume=float(raw.get('volume24h', 0)),
        )

    # Use Cython or Rust extension for 10-100x speedup
```

### 4.2 Tick Data Storage and Retrieval

**For Backtesting and Analysis** â­â­â­:
```python
# Use columnar storage (Parquet) for fast analytics
import pyarrow as pa
import pyarrow.parquet as pq

class TickStorage:
    def __init__(self, path: str):
        self.path = path
        self.schema = pa.schema([
            ('timestamp', pa.timestamp('us')),
            ('symbol', pa.string()),
            ('price', pa.float64()),
            ('volume', pa.float64()),
        ])

    def append_batch(self, ticks: list[Tick]):
        """Write batch of ticks (efficient)"""
        table = pa.Table.from_pydict({
            'timestamp': [t.timestamp for t in ticks],
            'symbol': [t.symbol for t in ticks],
            'price': [t.price for t in ticks],
            'volume': [t.volume for t in ticks],
        }, schema=self.schema)

        pq.write_to_dataset(table, root_path=self.path, partition_cols=['symbol'])
```

**For Real-Time Access** â­â­â­:
```python
# Use Redis Timeseries for fast tick lookups
import redis.asyncio as redis

async def store_tick(tick: Tick):
    key = f"ticks:{tick.symbol}"
    await redis_client.ts().add(key, tick.timestamp, tick.price)

async def get_recent_ticks(symbol: str, seconds: int = 60) -> list[Tick]:
    """Get last N seconds of ticks (fast)"""
    key = f"ticks:{symbol}"
    end = int(time.time() * 1000)
    start = end - (seconds * 1000)
    return await redis_client.ts().range(key, start, end)
```

### 4.3 Feature Computation Pipeline Optimization

**Problem**: Computing indicators (EMA, RSI, etc.) on every tick is expensive
**Solution**: Incremental computation + caching

```python
class IncrementalEMA:
    """Compute EMA incrementally (O(1) per update vs O(N))"""
    def __init__(self, period: int):
        self.period = period
        self.alpha = 2 / (period + 1)
        self.ema = None

    def update(self, price: float) -> float:
        if self.ema is None:
            self.ema = price
        else:
            self.ema = self.alpha * price + (1 - self.alpha) * self.ema
        return self.ema
```

**Use NumPy vectorization for batch processing** â­â­â­â­:
```python
import numpy as np

def compute_indicators_batch(prices: np.ndarray) -> dict:
    """Compute multiple indicators in one pass (SIMD)"""
    ema_20 = np.convolve(prices, np.ones(20)/20, mode='valid')
    ema_50 = np.convolve(prices, np.ones(50)/50, mode='valid')
    rsi = compute_rsi_vectorized(prices)  # NumPy implementation

    return {'ema_20': ema_20, 'ema_50': ema_50, 'rsi': rsi}
    # 10-100x faster than loop-based computation
```

### 4.4 Real-Time vs Batch Processing Tradeoffs

**Decision Matrix**:

| Component | Current | Recommendation | Rationale |
|-----------|---------|----------------|-----------|
| Market data ingestion | Real-time (WebSocket) | âœ… Keep real-time | Latency critical |
| Indicator computation | Real-time (on every tick) | âš ï¸ Hybrid: Real-time for fast indicators, batch for slow | Balance latency vs CPU |
| Risk checks | Real-time (on every order) | âœ… Keep real-time | Safety critical |
| Portfolio accounting | Real-time (on every fill) | âœ… Keep real-time | Accuracy critical |
| Performance analytics | Batch (periodic) | âœ… Keep batch | Not latency-sensitive |
| Backtesting | Batch | âœ… Keep batch | Offline process |

---

## 5. RELIABILITY & RESILIENCE

### 5.1 Failover Mechanisms

**Current State**: Single WebSocket connection with reconnection logic
**Improvement**: Dual WebSocket connections with failover

```python
class DualWebSocketManager:
    """Maintain two connections to different endpoints for redundancy"""
    def __init__(self):
        self.primary = BybitPublicWebSocket(endpoint="wss://stream-testnet.bybit.com")
        self.secondary = BybitPublicWebSocket(endpoint="wss://stream-testnet-backup.bybit.com")
        self.active = "primary"

    async def connect(self):
        await asyncio.gather(
            self.primary.connect(),
            self.secondary.connect()
        )

    async def _health_check(self):
        """Switch to secondary if primary is unhealthy"""
        while True:
            await asyncio.sleep(5.0)

            if self.active == "primary" and not self.primary.is_healthy():
                logger.warning("Primary WebSocket unhealthy, switching to secondary")
                self.active = "secondary"
            elif self.active == "secondary" and self.primary.is_healthy():
                logger.info("Primary WebSocket recovered, switching back")
                self.active = "primary"
```

### 5.2 State Recovery After Crashes

**Current State**: State stored in Redis
**Improvement**: Add checkpoint/restore functionality

```python
class StateCheckpoint:
    """Periodic state snapshots for fast recovery"""

    async def create_checkpoint(self):
        """Save critical state to Redis"""
        checkpoint = {
            'positions': self._accounting.get_positions(),
            'orders': self._order_manager.get_open_orders(),
            'equity': self._accounting.get_equity(),
            'timestamp': time.time(),
        }
        await redis_client.set('checkpoint:latest', json.dumps(checkpoint))

    async def restore_checkpoint(self):
        """Restore state from last checkpoint"""
        data = await redis_client.get('checkpoint:latest')
        if data:
            checkpoint = json.loads(data)
            # Restore positions, orders, etc.
            logger.info(f"Restored state from checkpoint at {checkpoint['timestamp']}")
```

### 5.3 Exchange Reconnection Strategies

**Current Code Analysis**: HEAN has basic exponential backoff
**Improvement**: Add jitter and circuit breaker

```python
class ReconnectionStrategy:
    def __init__(self):
        self.attempt = 0
        self.max_attempts = 10
        self.base_delay = 1.0
        self.max_delay = 60.0
        self.circuit_breaker_threshold = 5  # Open circuit after 5 consecutive failures

    def get_next_delay(self) -> float:
        """Exponential backoff with jitter"""
        delay = min(self.base_delay * (2 ** self.attempt), self.max_delay)
        jitter = random.uniform(0, delay * 0.1)  # Â±10% jitter
        return delay + jitter

    def should_reconnect(self) -> bool:
        """Circuit breaker logic"""
        if self.attempt >= self.circuit_breaker_threshold:
            logger.error("Circuit breaker OPEN - too many reconnection failures")
            return False
        return self.attempt < self.max_attempts

    def on_success(self):
        self.attempt = 0  # Reset on successful connection

    def on_failure(self):
        self.attempt += 1
```

### 5.4 Order State Reconciliation

**Problem**: Orders can get out of sync (exchange vs local state)
**Solution**: Periodic reconciliation

```python
class OrderReconciliation:
    """Reconcile local order state with exchange"""

    async def reconcile(self):
        """Compare local orders with exchange orders"""
        local_orders = self._order_manager.get_open_orders()
        exchange_orders = await self._bybit_http.get_open_orders()

        local_ids = {o.order_id for o in local_orders}
        exchange_ids = {o['orderId'] for o in exchange_orders}

        # Orders on exchange but not locally (missed fills?)
        missing_local = exchange_ids - local_ids
        if missing_local:
            logger.warning(f"Found {len(missing_local)} orders on exchange not in local state")
            for order_id in missing_local:
                # Fetch order details and update local state
                order = await self._bybit_http.get_order(order_id)
                self._order_manager.update_order(order)

        # Orders locally but not on exchange (canceled but not updated?)
        missing_exchange = local_ids - exchange_ids
        if missing_exchange:
            logger.warning(f"Found {len(missing_exchange)} local orders not on exchange")
            for order_id in missing_exchange:
                # Mark as canceled
                self._order_manager.cancel_order(order_id)
```

### 5.5 Split-Brain Prevention

**Problem**: Multiple instances of HEAN trading simultaneously
**Solution**: Distributed lock using Redis

```python
import redis.asyncio as redis

class TradingLock:
    """Distributed lock to prevent multiple instances"""

    def __init__(self, redis_client: redis.Redis, lock_key: str = "hean:trading_lock"):
        self.redis = redis_client
        self.lock_key = lock_key
        self.lock_timeout = 30  # seconds
        self.instance_id = str(uuid.uuid4())

    async def acquire(self) -> bool:
        """Try to acquire trading lock"""
        acquired = await self.redis.set(
            self.lock_key,
            self.instance_id,
            nx=True,  # Only set if not exists
            ex=self.lock_timeout
        )

        if acquired:
            logger.info(f"Acquired trading lock: {self.instance_id}")
            # Start heartbeat to renew lock
            asyncio.create_task(self._heartbeat())
            return True
        else:
            current_owner = await self.redis.get(self.lock_key)
            logger.error(f"Failed to acquire lock - owned by {current_owner}")
            return False

    async def _heartbeat(self):
        """Renew lock periodically"""
        while True:
            await asyncio.sleep(self.lock_timeout / 2)
            current = await self.redis.get(self.lock_key)
            if current == self.instance_id:
                await self.redis.expire(self.lock_key, self.lock_timeout)
            else:
                logger.error("Lost trading lock!")
                # Trigger graceful shutdown
                break
```

---

## 6. ADVANCED OPTIMIZATIONS (Future Roadmap)

### 6.1 Rust/C++ Extensions for Hot Paths â­â­â­â­â­

**Highest Impact Areas**:
1. **Orderbook reconstruction** (10-100x speedup)
2. **Indicator computation** (10-50x speedup)
3. **JSON parsing** (5-20x speedup with simd-json)

**Example: Rust Orderbook**:
```rust
// src/rust_orderbook/lib.rs
use pyo3::prelude::*;

#[pyclass]
struct OrderBook {
    bids: Vec<(f64, f64)>,  // (price, size)
    asks: Vec<(f64, f64)>,
}

#[pymethods]
impl OrderBook {
    #[new]
    fn new() -> Self {
        OrderBook {
            bids: Vec::with_capacity(200),
            asks: Vec::with_capacity(200),
        }
    }

    fn update_bid(&mut self, price: f64, size: f64) {
        // Binary search + insert (O(log N))
        match self.bids.binary_search_by(|&(p, _)| p.partial_cmp(&price).unwrap()) {
            Ok(idx) => self.bids[idx].1 = size,
            Err(idx) => self.bids.insert(idx, (price, size)),
        }
    }

    fn get_best_bid(&self) -> Option<f64> {
        self.bids.last().map(|(p, _)| *p)
    }
}

#[pymodule]
fn rust_orderbook(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_class::<OrderBook>()?;
    Ok(())
}
```

**Build**:
```bash
pip install maturin
cd rust_orderbook
maturin develop --release
```

**Use in Python**:
```python
from rust_orderbook import OrderBook

book = OrderBook()
book.update_bid(50000.0, 1.5)
book.get_best_bid()  # 10-100x faster than pure Python
```

### 6.2 Kernel Bypass Networking (DPDK/io_uring)

**Impact**: 50-90% latency reduction for network I/O
**Complexity**: Very High
**Recommendation**: Only if you need <100Î¼s latencies (not applicable for crypto exchanges)

**Reason**: Crypto exchanges have 5-20ms inherent latency, so optimizing client-side networking below 1ms has diminishing returns.

### 6.3 FPGA Acceleration

**Status**: Not recommended for HEAN
**Reason**:
- FPGA achieves 3-13ns latency (overkill for crypto)
- Development cost: $500k-$2M
- Crypto exchanges operate in millisecond range
- Better ROI from software optimizations

---

## 7. IMPLEMENTATION PRIORITY MATRIX

### Phase 1: Quick Wins (1-2 weeks) â­â­â­â­â­

| Optimization | Effort | Impact | Risk | Priority |
|--------------|--------|--------|------|----------|
| Install uvloop | 1 hour | 2-4x throughput | Very Low | ğŸ”´ CRITICAL |
| Replace json with orjson | 4 hours | 20-50% faster parsing | Very Low | ğŸ”´ CRITICAL |
| HTTP connection pooling | 4 hours | 30-50% faster orders | Low | ğŸŸ  HIGH |
| Optimize Docker image | 8 hours | 20-30% faster startup | Low | ğŸŸ  HIGH |
| Deploy to Singapore AWS | 1 day | <1ms latency to Bybit | Medium | ğŸŸ  HIGH |
| Add latency metrics | 1 day | Visibility into bottlenecks | Low | ğŸŸ  HIGH |

**Total: 3 days of work â†’ 2-5x performance improvement**

### Phase 2: Medium Complexity (2-4 weeks) â­â­â­â­

| Optimization | Effort | Impact | Risk | Priority |
|--------------|--------|--------|------|----------|
| Incremental orderbook updates | 1 week | 10-100x faster book | Medium | ğŸŸ  HIGH |
| EventBus inline handlers | 1 week | 50-80% signal latency | Medium | ğŸŸ  HIGH |
| GC tuning + object pooling | 1 week | Eliminate GC pauses | Medium | ğŸŸ¡ MEDIUM |
| Redis optimization | 3 days | 2-3x faster state ops | Low | ğŸŸ¡ MEDIUM |
| TCP tuning | 2 days | 10-20% network perf | Low | ğŸŸ¡ MEDIUM |

**Total: 3-4 weeks â†’ Another 2-3x improvement**

### Phase 3: Major Refactoring (1-3 months) â­â­â­

| Optimization | Effort | Impact | Risk | Priority |
|--------------|--------|--------|------|----------|
| Multi-process architecture | 4 weeks | Full CPU utilization | High | ğŸŸ¡ MEDIUM |
| Rust orderbook extension | 2 weeks | 10-100x orderbook | Medium | ğŸŸ¡ MEDIUM |
| Dual WebSocket failover | 1 week | Improved reliability | Medium | ğŸŸ¡ MEDIUM |
| State checkpoint/recovery | 1 week | Fast crash recovery | Medium | ğŸŸ¡ MEDIUM |

**Total: 2-3 months â†’ 3-5x improvement + reliability**

---

## 8. COST-BENEFIT ANALYSIS

### Infrastructure Costs

| Component | Monthly Cost | Latency Benefit | ROI |
|-----------|--------------|-----------------|-----|
| AWS c7gn.xlarge (Singapore) | $144 | 10-50ms â†’ <1ms | â­â­â­â­â­ |
| Enhanced monitoring (Datadog) | $50 | Visibility | â­â­â­â­ |
| Redis cluster (production) | $100 | Higher throughput | â­â­â­ |
| **Total** | **$294/month** | - | - |

### Development Costs

| Phase | Engineer Time | Opportunity Cost | Performance Gain |
|-------|---------------|------------------|------------------|
| Phase 1 (Quick Wins) | 3 days | $2,400 | 2-5x |
| Phase 2 (Medium) | 4 weeks | $12,800 | 2-3x |
| Phase 3 (Major) | 3 months | $38,400 | 3-5x |

**Total 6-Month Investment**: ~$54,000 (infrastructure + dev time)
**Expected Performance**: 10-30x improvement in latency + 5-10x throughput

### Trading Performance Impact

**Scenario**: Market making on BTCUSDT with $100k capital

| Metric | Before | After Optimization | Improvement |
|--------|--------|-------------------|-------------|
| Tick-to-trade latency | 50ms | 5ms | 10x faster |
| Orders per second | 10 | 100 | 10x throughput |
| Fill rate (maker) | 30% | 60% | 2x fills |
| Adverse selection cost | $100/day | $30/day | 70% reduction |
| Net PnL improvement | - | +$200-500/day | ROI: weeks |

**Break-even**: 2-4 weeks of improved trading performance

---

## 9. MONITORING & VALIDATION

### 9.1 Key Performance Indicators (KPIs)

**Latency Metrics**:
```python
# Add to Prometheus metrics
tick_to_signal_latency = Histogram(
    'tick_to_signal_seconds',
    'Latency from tick arrival to signal generation',
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1]
)

signal_to_order_latency = Histogram(
    'signal_to_order_seconds',
    'Latency from signal to order submission',
    buckets=[0.01, 0.05, 0.1, 0.2, 0.5]
)

order_ack_latency = Histogram(
    'order_ack_seconds',
    'Latency from order submission to exchange ack',
    buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1.0]
)
```

**Throughput Metrics**:
```python
ticks_processed = Counter('ticks_processed_total', 'Total ticks processed')
signals_generated = Counter('signals_generated_total', 'Total signals generated')
orders_submitted = Counter('orders_submitted_total', 'Total orders submitted')
orders_filled = Counter('orders_filled_total', 'Total orders filled')
```

**System Health Metrics**:
```python
gc_pause_duration = Histogram('gc_pause_seconds', 'GC pause duration')
websocket_reconnects = Counter('websocket_reconnects_total', 'WebSocket reconnections')
eventbus_queue_depth = Gauge('eventbus_queue_depth', 'EventBus queue depth')
redis_latency = Histogram('redis_latency_seconds', 'Redis operation latency')
```

### 9.2 Regression Testing

**Latency Benchmark Suite**:
```python
# tests/benchmarks/test_latency.py
import pytest
import time

@pytest.mark.benchmark
def test_tick_processing_latency(benchmark):
    """Ensure tick processing stays under 5ms"""

    def process_tick():
        tick = Tick(symbol="BTCUSDT", price=50000.0, timestamp=time.time())
        # Process through pipeline
        strategy.on_tick(tick)

    result = benchmark(process_tick)
    assert result.stats.mean < 0.005  # 5ms threshold

@pytest.mark.benchmark
def test_order_submission_latency(benchmark):
    """Ensure order submission stays under 50ms"""

    def submit_order():
        request = OrderRequest(symbol="BTCUSDT", side="BUY", size=0.001, ...)
        # Submit to exchange
        asyncio.run(router.submit_order(request))

    result = benchmark(submit_order)
    assert result.stats.mean < 0.05  # 50ms threshold
```

### 9.3 A/B Testing

**Compare Before/After Performance**:
```python
# Run with and without optimization
# Example: uvloop vs asyncio

def benchmark_event_processing():
    # Without uvloop
    import asyncio
    start = time.time()
    asyncio.run(process_10000_events())
    baseline = time.time() - start

    # With uvloop
    import uvloop
    uvloop.install()
    start = time.time()
    asyncio.run(process_10000_events())
    optimized = time.time() - start

    improvement = (baseline - optimized) / baseline * 100
    print(f"Improvement: {improvement:.1f}%")
```

---

## 10. CONCLUSION & NEXT STEPS

### Summary

HEAN has a solid foundation with:
- âœ… Event-driven architecture
- âœ… Async-first design
- âœ… Multi-priority event bus
- âœ… WebSocket market data
- âœ… Docker containerization

**Optimization Potential**: 10-30x latency improvement and 5-10x throughput increase with moderate effort.

### Immediate Action Plan (Next 7 Days)

**Day 1-2: Quick Wins**
1. Install uvloop (1 hour)
2. Replace json with orjson (4 hours)
3. Optimize httpx connection pooling (4 hours)
4. Add latency metrics to Prometheus (8 hours)

**Day 3-4: Infrastructure**
5. Deploy to AWS Singapore (c7gn.xlarge) (8 hours)
6. Configure TCP tuning on host (2 hours)
7. Optimize Docker image (jemalloc, bytecode compilation) (6 hours)

**Day 5-7: Validation**
8. Run latency benchmarks and compare (4 hours)
9. Monitor production performance (ongoing)
10. Document results and plan Phase 2 (4 hours)

**Expected Outcome**: 3-5x performance improvement in 1 week

### Long-Term Roadmap (6 Months)

**Month 1**: Phase 1 complete (quick wins)
**Month 2-3**: Phase 2 (incremental orderbook, EventBus optimization, GC tuning)
**Month 4-6**: Phase 3 (multi-process architecture, Rust extensions, advanced monitoring)

**Final Performance Target**:
- Tick-to-trade latency: <5ms (P99)
- Order submission latency: <20ms (P99)
- Throughput: 100+ orders/second
- Reliability: 99.9% uptime

### References & Further Reading

**Performance Optimization**:
- [uvloop: Blazing fast Python networking](https://magic.io/blog/uvloop-blazing-fast-python-networking/)
- [FastAPI performance optimization guide](https://fastapi.tiangolo.com/deployment/concepts/)
- [Python GC optimization for low-latency systems](https://instagram-engineering.com/dismissing-python-garbage-collection-at-instagram-4dca40b29172)

**Trading Infrastructure**:
- [AWS: Optimize tick-to-trade latency for digital assets](https://aws.amazon.com/blogs/web3/optimize-tick-to-trade-latency-for-digital-assets-exchanges-and-trading-platforms-on-aws/)
- [Low Latency Trading Systems Guide](https://www.tuvoc.com/blog/low-latency-trading-systems-guide/)
- [Kernel Bypass Techniques in Linux for HFT](https://lambdafunc.medium.com/kernel-bypass-techniques-in-linux-for-high-frequency-trading-a-deep-dive-de347ccd5407)

**Rust/C++ Extensions**:
- [PyO3: Rust bindings for Python](https://pyo3.rs/)
- [Maturin: Build and publish Rust extensions](https://www.maturin.rs/)
- [Switching from Python to Rust: HFT Case Study](https://dev.to/frankdotdev/switching-from-python-to-rust-a-high-frequency-trading-case-study-34hc)

---

**Document Version**: 1.0
**Last Updated**: 2026-02-06
**Author**: HEAN Infrastructure Team
**Review Cycle**: Quarterly
